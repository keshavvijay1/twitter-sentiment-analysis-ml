{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e4aa57-40a9-4154-adaa-ccdf4d9a4f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully read the file with latin1 encoding.\n",
      "üìä DataFrame Head:\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "| target |     id     |             date             |   flag   |       user      |                                                         text                                                        |\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|   0    | 1467810369 | Mon Apr 06 22:19:45 PDT 2009 | NO_QUERY | _TheSpecialOne_ | @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D |\n",
      "|   0    | 1467810672 | Mon Apr 06 22:19:49 PDT 2009 | NO_QUERY |  scotthamilton  |   is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   |\n",
      "|   0    | 1467810917 | Mon Apr 06 22:19:53 PDT 2009 | NO_QUERY |     mattycus    |              @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds              |\n",
      "|   0    | 1467811184 | Mon Apr 06 22:19:57 PDT 2009 | NO_QUERY |     ElleCTF     |                                   my whole body feels itchy and like its on fire                                    |\n",
      "|   0    | 1467811193 | Mon Apr 06 22:19:57 PDT 2009 | NO_QUERY |      Karoli     |   @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.    |\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "üîç Checking for missing values...\n",
      "target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n",
      "üìà Distribution of target values:\n",
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "‚öôÔ∏è Applying stemming to the text column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1601/1601 [02:56<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Vectorizing the text data...\n",
      "‚úÇÔ∏è Splitting the data into training and test sets...\n",
      "ü§ñ Training the Logistic Regression model...\n",
      "üîç Predicting on test data...\n",
      "üéØ Accuracy: 76.85%\n",
      "üîÆ Welcome to the Tweet Sentiment Analyzer!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[36mEnter a tweet (or type 'exit' to quit):  k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[36mEnter a tweet (or type 'exit' to quit):  fuck\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative üò†\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[36mEnter a tweet (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style, init\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# File path\n",
    "csv_file_path = \"E:\\\\New folder\\\\archive (2)\\\\training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# Try to read the dataset with different encodings\n",
    "encodings = ['latin1', 'iso-8859-1', 'cp1252']\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        twitter_data = pd.read_csv(csv_file_path, encoding=encoding)\n",
    "        print(Fore.GREEN + f\"‚úÖ Successfully read the file with {encoding} encoding.\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(Fore.RED + f\"‚ùå Failed to read the file with {encoding} encoding.\")\n",
    "\n",
    "# Rename columns\n",
    "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "twitter_data = pd.read_csv(csv_file_path, names=column_names, encoding='ISO-8859-1')\n",
    "\n",
    "# Display head of the dataframe in a table\n",
    "print(Fore.YELLOW + \"üìä DataFrame Head:\")\n",
    "table = PrettyTable()\n",
    "table.field_names = twitter_data.head(5).columns.tolist()\n",
    "for row in twitter_data.head(5).values:\n",
    "    table.add_row(row)\n",
    "print(table)\n",
    "\n",
    "# Check for missing values\n",
    "print(Fore.YELLOW + \"üîç Checking for missing values...\")\n",
    "missing_values = twitter_data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Check the distribution of target values\n",
    "print(Fore.YELLOW + \"üìà Distribution of target values:\")\n",
    "target_distribution = twitter_data['target'].value_counts()\n",
    "print(target_distribution)\n",
    "\n",
    "# Replace target values (4 -> 1)\n",
    "twitter_data.replace({'target': {4: 1}}, inplace=True)\n",
    "\n",
    "# Improved Stemming function using SnowballStemmer\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "def stemming(content):\n",
    "    content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    content = content.lower()\n",
    "    content = content.split()\n",
    "    content = [stemmer.stem(word) for word in content if word not in stop_words]\n",
    "    content = ' '.join(content)\n",
    "    return content\n",
    "\n",
    "# Apply stemming to the text column in batches\n",
    "print(Fore.YELLOW + \"‚öôÔ∏è Applying stemming to the text column...\")\n",
    "batch_size = 1000\n",
    "num_batches = len(twitter_data) // batch_size + 1\n",
    "stemmed_texts = []\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(twitter_data))\n",
    "    batch_texts = [stemming(text) for text in twitter_data['text'].iloc[start_idx:end_idx]]\n",
    "    stemmed_texts.extend(batch_texts)\n",
    "\n",
    "twitter_data['text'] = stemmed_texts\n",
    "\n",
    "# Vectorize the text data with HashingVectorizer\n",
    "print(Fore.YELLOW + \"üìè Vectorizing the text data...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(twitter_data['text'])\n",
    "y = twitter_data['target'].values\n",
    "\n",
    "# Split the data\n",
    "print(Fore.YELLOW + \"‚úÇÔ∏è Splitting the data into training and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "print(Fore.YELLOW + \"ü§ñ Training the Logistic Regression model...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "print(Fore.YELLOW + \"üîç Predicting on test data...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(Fore.GREEN + f\"üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Function to predict sentiment of a tweet\n",
    "def predict_sentiment(tweet):\n",
    "    tweet = stemming(tweet)\n",
    "    tweet_vector = vectorizer.transform([tweet])\n",
    "    prediction = model.predict(tweet_vector)\n",
    "    sentiment = \"Positive üòä\" if prediction == 1 else \"Negative üò†\"\n",
    "    return sentiment\n",
    "\n",
    "# User-friendly interaction\n",
    "def main():\n",
    "    print(Fore.YELLOW + \"üîÆ Welcome to the Tweet Sentiment Analyzer!\")\n",
    "    while True:\n",
    "        tweet = input(Fore.CYAN + \"Enter a tweet (or type 'exit' to quit): \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(Fore.YELLOW + \"üëã Goodbye!\")\n",
    "            break\n",
    "        sentiment = predict_sentiment(tweet)\n",
    "        color = Fore.GREEN if \"Positive\" in sentiment else Fore.RED\n",
    "        print(color + f\"Sentiment: {sentiment}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16622a90-2fe6-4cb8-987a-18b08dd55e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style, init\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f272b31d-5d09-4aaa-af34-2f4edd74cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully read the file with latin1 encoding.\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "csv_file_path = \"E:\\\\New folder\\\\archive (2)\\\\training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# Try to read the dataset with different encodings\n",
    "encodings = ['latin1', 'iso-8859-1', 'cp1252']\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        twitter_data = pd.read_csv(csv_file_path, encoding=encoding)\n",
    "        print(Fore.GREEN + f\"‚úÖ Successfully read the file with {encoding} encoding.\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(Fore.RED + f\"‚ùå Failed to read the file with {encoding} encoding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72978b1-3b4a-433e-b0f6-666f0aa06440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "twitter_data = pd.read_csv(csv_file_path, names=column_names, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6728c6-43de-4759-a901-4a8eb3f71263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame Head:\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "| target |     id     |             date             |   flag   |       user      |                                                         text                                                        |\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|   0    | 1467810369 | Mon Apr 06 22:19:45 PDT 2009 | NO_QUERY | _TheSpecialOne_ | @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D |\n",
      "|   0    | 1467810672 | Mon Apr 06 22:19:49 PDT 2009 | NO_QUERY |  scotthamilton  |   is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   |\n",
      "|   0    | 1467810917 | Mon Apr 06 22:19:53 PDT 2009 | NO_QUERY |     mattycus    |              @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds              |\n",
      "|   0    | 1467811184 | Mon Apr 06 22:19:57 PDT 2009 | NO_QUERY |     ElleCTF     |                                   my whole body feels itchy and like its on fire                                    |\n",
      "|   0    | 1467811193 | Mon Apr 06 22:19:57 PDT 2009 | NO_QUERY |      Karoli     |   @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.    |\n",
      "+--------+------------+------------------------------+----------+-----------------+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Display head of the dataframe in a table\n",
    "print(Fore.YELLOW + \"üìä DataFrame Head:\")\n",
    "table = PrettyTable()\n",
    "table.field_names = twitter_data.head(5).columns.tolist()\n",
    "for row in twitter_data.head(5).values:\n",
    "    table.add_row(row)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ca1e39-f493-4fad-b19b-bea4dfe0d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for missing values...\n",
      "target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(Fore.YELLOW + \"üîç Checking for missing values...\")\n",
    "missing_values = twitter_data.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b676ea6e-a39b-4966-90fe-7672271d6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Distribution of target values:\n",
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of target values\n",
    "print(Fore.YELLOW + \"üìà Distribution of target values:\")\n",
    "target_distribution = twitter_data['target'].value_counts()\n",
    "print(target_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656473e6-3acf-4efe-a8ea-8f03b77082b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target values (4 -> 1)\n",
    "twitter_data.replace({'target': {4: 1}}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c300ce-0bfa-447f-b241-f77e8aaa5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Stemming function using SnowballStemmer\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "def stemming(content):\n",
    "    content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    content = content.lower()\n",
    "    content = content.split()\n",
    "    content = [stemmer.stem(word) for word in content if word not in stop_words]\n",
    "    content = ' '.join(content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c231d035-1b6e-4c4d-ad5a-307853af0a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Applying stemming to the text column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1601/1601 [02:00<00:00, 13.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply stemming to the text column in batches\n",
    "print(Fore.YELLOW + \"‚öôÔ∏è Applying stemming to the text column...\")\n",
    "batch_size = 1000\n",
    "num_batches = len(twitter_data) // batch_size + 1\n",
    "stemmed_texts = []\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(twitter_data))\n",
    "    batch_texts = [stemming(text) for text in twitter_data['text'].iloc[start_idx:end_idx]]\n",
    "    stemmed_texts.extend(batch_texts)\n",
    "\n",
    "twitter_data['text'] = stemmed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770ab03c-251f-4e3e-a838-a5737d1cb8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Vectorizing the text data...\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data with TfidfVectorizer\n",
    "print(Fore.YELLOW + \"üìè Vectorizing the text data...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(twitter_data['text'])\n",
    "y = twitter_data['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cea86f-ae9b-479d-a814-7976254307e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Splitting the data into training and test sets...\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "print(Fore.YELLOW + \"‚úÇÔ∏è Splitting the data into training and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe06bcd7-f2e8-4418-b011-c66f00991a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Training the Logistic Regression model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "print(Fore.YELLOW + \"ü§ñ Training the Logistic Regression model...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977ef631-1a4c-488e-b6c8-46dfd58e8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Predicting on test data...\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print(Fore.YELLOW + \"üîç Predicting on test data...\")\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c988f5bc-8ebb-4e3f-8af1-1caf3ff874bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Accuracy: 76.85%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(Fore.GREEN + f\"üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Function to predict sentiment of a tweet\n",
    "def predict_sentiment(tweet):\n",
    "    tweet = stemming(tweet)\n",
    "    tweet_vector = vectorizer.transform([tweet])\n",
    "    prediction = model.predict(tweet_vector)\n",
    "    sentiment = \"Positive üòä\" if prediction == 1 else \"Negative üò†\"\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf280ff-9374-411a-8d71-0c061cc90bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Welcome to the Tweet Sentiment Analyzer!\n"
     ]
    }
   ],
   "source": [
    "# User-friendly interaction\n",
    "def main():\n",
    "    print(Fore.YELLOW + \"üîÆ Welcome to the Tweet Sentiment Analyzer!\")\n",
    "    while True:\n",
    "        tweet = input(Fore.CYAN + \"Enter a tweet (or type 'exit' to quit): \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(Fore.YELLOW + \"üëã Goodbye!\")\n",
    "            break\n",
    "        sentiment = predict_sentiment(tweet)\n",
    "        color = Fore.GREEN if \"Positive\" in sentiment else Fore.RED\n",
    "        print(color + f\"Sentiment: {sentiment}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe333e-8780-4944-94f9-1a09e7c54e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
